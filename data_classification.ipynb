{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9746,"status":"ok","timestamp":1713874822529,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"HhfzqLkk5Ens"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","\n","This script performs the following tasks:\n","\n","1. Uses as input the weights from a run of the Convolutional Variational Autoencoder (CVAE) from the TensorFlow tutorial.\n","2. Uses as input the latent space of the MNIST dataset encoded through CVAE.\n","\n","The script implements both the encoder and decoder as functions.\n","\n","Authors:\n","- Gabriel Turinici\n","- Tiffany Zeitoun\n","- Dan Winzsman\n","- Benjamin Krief\n","\"\"\"\n","\n","# Import necessary libraries for deep learning, visualization, and data processing\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm  # For displaying progress bars during training or data loading\n","import pandas as pd  # For handling data operations and analysis"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3611,"status":"ok","timestamp":1713874826135,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"kSu7RdVx5j98","outputId":"927cad97-808d-4547-c303-d7edfd992a95"},"outputs":[{"name":"stdout","output_type":"stream","text":["download necessary files\n","downloading my_checkpoint.index from https://github.com/gabriel-turinici/Huber-energy-measure-quantization/raw/main/diversity_in_generative_ai/\n","downloading reparam.npz from https://github.com/gabriel-turinici/Huber-energy-measure-quantization/raw/main/diversity_in_generative_ai/\n","downloading my_checkpoint.data-00000-of-00001 from https://github.com/gabriel-turinici/Huber-energy-measure-quantization/raw/main/diversity_in_generative_ai/\n"]}],"source":["# Print a message indicating the download process is starting\n","print('Downloading necessary files')\n","\n","# Import the requests library for handling file downloads\n","import requests\n","\n","# Base URL for downloading the required files\n","base_url = 'https://github.com/gabriel-turinici/Huber-energy-measure-quantization/raw/main/diversity_in_generative_ai/'\n","\n","# List of file names to be downloaded\n","files_names = ['my_checkpoint.index', 'reparam.npz', 'my_checkpoint.data-00000-of-00001']\n","\n","# Loop through the list of files and download each one\n","for fl in files_names:\n","    print(f'Downloading {fl} from {base_url}')  # Inform the user which file is being downloaded\n","    response = requests.get(base_url + fl)  # Send a request to download the file\n","    with open(fl, mode=\"wb\") as file:  # Open the file in write-binary mode\n","        file.write(response.content)  # Write the downloaded content to the file\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":316,"status":"ok","timestamp":1713874828715,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"GFjvUgVx5mYK"},"outputs":[],"source":["def preprocess_images(images):\n","    \"\"\"\n","    Preprocess the input images by reshaping and normalizing pixel values.\n","\n","    Args:\n","        images (numpy array): Array of images to be preprocessed.\n","\n","    Returns:\n","        numpy array: Preprocessed images, reshaped to (28, 28, 1) and normalized to float32.\n","    \"\"\"\n","    # Reshape the images to add a channel dimension (28x28 images with 1 channel)\n","    images = images.reshape((images.shape[0], 28, 28, 1)) / 255.0  # Normalize pixel values to [0, 1] range\n","    \n","    return images.astype('float32')\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1388,"status":"ok","timestamp":1713875764303,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"jPnFpqXw5pJv","outputId":"40faaaae-e514-4e54-c0ae-b4091348c941"},"outputs":[{"name":"stdout","output_type":"stream","text":["load mnist, preprocess images\n","train_image.shape= (60000, 28, 28, 1)\n"]}],"source":["# Load and preprocess the MNIST dataset\n","print('Loading MNIST dataset and preprocessing images')\n","\n","# Load the MNIST dataset from TensorFlow's built-in datasets (we only need the images, not the labels)\n","(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n","\n","# Define a function to preprocess MNIST images\n","def preprocess_images(images):\n","    \"\"\"\n","    Preprocess the input images by reshaping, normalizing, and binarizing pixel values.\n","\n","    Args:\n","        images (numpy array): Array of images to preprocess.\n","\n","    Returns:\n","        numpy array: Preprocessed images, reshaped to (28, 28, 1), normalized and binarized.\n","    \"\"\"\n","    # Reshape the images to add a channel dimension (28x28 images with 1 channel) and normalize pixel values\n","    images = images.reshape((images.shape[0], 28, 28, 1)) / 255.0\n","\n","    # Binarize pixel values: set values > 0.5 to 1.0 and others to 0.0\n","    return np.where(images > 0.5, 1.0, 0.0).astype('float32')\n","\n","# Preprocess the training and testing images\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","\n","# Display the shape of the preprocessed training images\n","print('Shape of preprocessed train_images:', train_images.shape)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":218,"status":"ok","timestamp":1713875766652,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"UzbTIj8h5sQm"},"outputs":[],"source":["N = 2  # Ambient dimension for latent space\n","\n","class CVAE(tf.keras.Model):\n","    \"\"\"Convolutional Variational Autoencoder (CVAE) class definition.\"\"\"\n","\n","    def __init__(self, latent_dim):\n","        \"\"\"\n","        Initialize the CVAE model.\n","        \n","        Args:\n","            latent_dim (int): Dimensionality of the latent space.\n","        \"\"\"\n","        super(CVAE, self).__init__()\n","        self.latent_dim = latent_dim\n","\n","        # Define the encoder network as a Sequential model\n","        self.encoder = tf.keras.Sequential(\n","            [\n","                # Input layer: shape of the input is (28, 28, 1) for MNIST images\n","                tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n","                \n","                # Flatten the input image into a 1D vector\n","                tf.keras.layers.Flatten(),\n","                \n","                # Add 5 fully connected layers with ReLU activation\n","                *[tf.keras.layers.Dense(28 * 28, activation='relu') for _ in range(5)],\n","                \n","                # The final layer outputs both mean and log variance for reparameterization\n","                tf.keras.layers.Dense(latent_dim + latent_dim)  # Output latent_dim for mean and logvar\n","            ]\n","        )\n","\n","        # Define the decoder network as a Sequential model\n","        self.decoder = tf.keras.Sequential(\n","            [\n","                # Input layer: latent space input of shape (latent_dim,)\n","                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n","\n","                # Flatten the latent vector (though it's already 1D in this case)\n","                tf.keras.layers.Flatten(),\n","\n","                # Add fully connected layers to reconstruct the original image\n","                *[tf.keras.layers.Dense(28 * 28, activation='relu') for _ in range(4)],\n","\n","                # The final layer reshapes the output back to (28, 28, 1) for the image\n","                tf.keras.layers.Dense(28 * 28),  # Output logits without activation\n","                tf.keras.layers.Reshape((28, 28, 1))  # Reshape the output to match the original image size\n","            ]\n","        )\n","\n","    @tf.function\n","    def sample(self, eps=None):\n","        \"\"\"\n","        Generate samples from the latent space by decoding random or given points.\n","\n","        Args:\n","            eps (tensor, optional): Latent space points. If not provided, random points are sampled.\n","        \n","        Returns:\n","            tensor: Decoded output from latent space points, typically images.\n","        \"\"\"\n","        if eps is None:\n","            # If no latent points are provided, generate random points from a normal distribution\n","            eps = tf.random.normal(shape=(100, self.latent_dim))\n","        \n","        # Decode the latent space points into images\n","        return self.decode(eps, apply_sigmoid=True)\n","\n","    def encode(self, x):\n","        \"\"\"\n","        Encodes the input images into the latent space.\n","        \n","        Args:\n","            x (tensor): Input image data.\n","        \n","        Returns:\n","            tuple: Mean and log variance of the encoded latent variables.\n","        \"\"\"\n","        # Split the encoder output into mean and log variance for the latent space\n","        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n","        return mean, logvar\n","\n","    def reparameterize(self, mean, logvar):\n","        \"\"\"\n","        Reparameterization trick to sample from a normal distribution using the mean and log variance.\n","        \n","        Args:\n","            mean (tensor): Mean of the latent variables.\n","            logvar (tensor): Log variance of the latent variables.\n","        \n","        Returns:\n","            tensor: Sampled latent variables.\n","        \"\"\"\n","        # Sample random noise from a standard normal distribution\n","        eps = tf.random.normal(shape=mean.shape)\n","        \n","        # Reparameterize to generate latent variables\n","        return eps * tf.exp(logvar * 0.5) + mean\n","\n","    def decode(self, z, apply_sigmoid=False):\n","        \"\"\"\n","        Decodes the latent variables back into images.\n","        \n","        Args:\n","            z (tensor): Latent variables.\n","            apply_sigmoid (bool, optional): Whether to apply the sigmoid function to the output.\n","        \n","        Returns:\n","            tensor: Decoded output, typically images.\n","        \"\"\"\n","        # Pass the latent variables through the decoder network to reconstruct images\n","        logits = self.decoder(z)\n","\n","        # Apply sigmoid activation if specified (useful for binary outputs)\n","        if apply_sigmoid:\n","            return tf.sigmoid(logits)\n","        \n","        return logits  # Return the raw logits if no sigmoid is applied\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1021,"status":"ok","timestamp":1713875769505,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"O4My3Inc5ueh","outputId":"662443d5-29d9-412f-9f7c-0964fa3688d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Create a new model instance\n","restore weights\n"]},{"data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7b4a914872b0>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["latent_dim = 2  # Set the dimensionality of the latent space\n","\n","# Create a new instance of the CVAE model\n","print('Creating a new CVAE model instance')\n","model = CVAE(latent_dim)\n","\n","# Generate a sample from the model using a random latent vector\n","res = model.sample(np.random.randn(1, latent_dim))  # Sampling from a random latent vector\n","\n","# Uncomment the following line if you want to generate and save images using a custom function\n","# generate_and_save_images(model, 0, test_sample)\n","\n","# Set the model's 'built' attribute to True to allow loading weights\n","model.built = True\n","\n","# Restore the model weights from a checkpoint\n","print('Restoring model weights from checkpoint')\n","\n","# Ensure that you are in the correct directory where the \"my_checkpoint.index\" file is located\n","model.load_weights(\"./my_checkpoint\")\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713875770367,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"KsvqIQAL5wlP"},"outputs":[],"source":["def decoder_function(latent, digit_size=28):\n","    \"\"\"\n","    Decoder function: Takes a latent vector as input and outputs a 28x28 image.\n","    \n","    Args:\n","        latent (tensor): Latent vector of shape (latent_dim,).\n","        digit_size (int): Size of the output image, default is 28 (for 28x28 image).\n","    \n","    Returns:\n","        tensor: Decoded image of shape (digit_size, digit_size).\n","    \"\"\"\n","    # Ensure the latent vector has the correct shape\n","    assert latent.shape == (latent_dim,), 'Error: input latent vector has incorrect shape'\n","    \n","    # Pass the latent vector through the model's decoder to generate the image\n","    x_decoded = model.sample(latent[None, :])  # Add batch dimension\n","    \n","    # Reshape the output into a 28x28 image\n","    digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n","    \n","    return digit\n","\n","\n","def encoder_function(image, digit_size=28):\n","    \"\"\"\n","    Encoder function: Takes an image as input and outputs a latent vector.\n","    \n","    Args:\n","        image (tensor): Input image of shape (digit_size, digit_size).\n","        digit_size (int): Size of the input image, default is 28 (for 28x28 image).\n","    \n","    Returns:\n","        tensor: Latent vector of shape (latent_dim,).\n","    \"\"\"\n","    # Ensure the input image has the correct shape\n","    assert image.shape == (digit_size, digit_size), 'Error: input image has incorrect shape'\n","    \n","    # Reshape the image to add the necessary batch and channel dimensions, then pass through the encoder\n","    res = model.encoder(tf.reshape(image, (1, digit_size, digit_size, 1)))  # Add batch and channel dimensions\n","    \n","    # Return the latent vector (taking only the first 'latent_dim' values)\n","    return res[0, :latent_dim]\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":313,"status":"ok","timestamp":1713875772306,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"byGNja-i5yg-"},"outputs":[],"source":["# Adjust the display options for NumPy arrays\n","np.set_printoptions(threshold=np.inf, linewidth=np.inf)  # Ensures full array output without truncation\n","\n","# Load the NumPy array from the 'reparam.npz' file\n","with np.load('reparam.npz', allow_pickle=True) as data:\n","    reparam = data['reparam']  # Extract the 'reparam' array from the file\n","\n","# Uncomment the line below if you wish to print the entire first element of the array\n","# print(reparam[0])"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"elapsed":239,"status":"ok","timestamp":1713882124354,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"7KvGNFpEVcAA"},"outputs":[],"source":["def create_list_sub_txt(huge_list, filepath=''):\n","    \"\"\"\n","    Save a list of sublists to a text file, each sublist written on a new line.\n","\n","    Args:\n","        huge_list (list of lists): The list containing sublists to save.\n","        filepath (str): The path where the file will be saved.\n","    \"\"\"\n","    with open(filepath, 'w') as file:\n","        # Write each sublist to the file, formatting it as a string\n","        for sublist in huge_list:\n","            file.write('[' + ', '.join(map(str, sublist)) + ']\\n')\n","\n","\n","def create_list_txt(simple_list, filepath=''):\n","    \"\"\"\n","    Save a simple list to a text file, all elements on the same line.\n","\n","    Args:\n","        simple_list (list): The list of elements to save.\n","        filepath (str): The path where the file will be saved.\n","    \"\"\"\n","    with open(filepath, 'w') as file:\n","        # Write the simple list to the file as a single line, separated by commas\n","        file.write(', '.join(map(str, simple_list)) + '\\n')\n"]},{"cell_type":"code","execution_count":104,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4012,"status":"ok","timestamp":1713883907268,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"NMVfJHLu53jC","outputId":"1e33e830-b13e-4f35-93b2-49b5c9af2cf6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import files\n","from google.colab import drive\n","import ast\n","import numpy as np\n","\n","# Mount Google Drive to access files\n","drive.mount('/content/drive')\n","\n","# Function to retrieve solutions from a file\n","def recup_solutions(filepath=''):\n","    \"\"\"\n","    Retrieve solutions from a file where each line contains a list of values.\n","\n","    Args:\n","        filepath (str): Path to the file containing the solutions.\n","\n","    Returns:\n","        list: A list of parsed solutions.\n","    \"\"\"\n","    with open(filepath, 'r') as file:\n","        # Parse each line as a Python object (list)\n","        solutions = [ast.literal_eval(line.strip()) for line in file]\n","    return solutions\n","\n","# Function to retrieve a list of floats from a comma-separated file\n","def recup_da_floats(filepath):\n","    \"\"\"\n","    Retrieve a list of float values from a file where values are comma-separated.\n","\n","    Args:\n","        filepath (str): Path to the file containing comma-separated float values.\n","\n","    Returns:\n","        list: A list of floats.\n","    \"\"\"\n","    with open(filepath, 'r') as file:\n","        # Split the file content by commas and convert to floats\n","        data = file.read().strip().split(',')\n","        float_list = [float(x) for x in data]\n","    return float_list\n","\n","# Filepaths for various data files\n","filepath_final_errors = '/content/drive/My Drive/Thesis/data/final_errors.txt'\n","filepath_deltas = '/content/drive/My Drive/Thesis/data/deltas.txt'\n","filepath_solutions = '/content/drive/My Drive/Thesis/data/solutions.txt'\n","filepath_reparam = '/content/drive/My Drive/Thesis/data/reparam.txt'\n","filepath_bonus = '/content/drive/My Drive/Thesis/data/bonus.txt'\n","\n","# Retrieve data from the respective files\n","final_errors = recup_da_floats(filepath_final_errors)  # List of final errors\n","deltas = recup_da_floats(filepath_deltas)  # List of deltas\n","solutions = recup_solutions(filepath_solutions)  # List of solutions (parsed)\n","reparam = np.array(recup_solutions(filepath_reparam))  # Convert reparam solutions to a numpy array\n","bonus = np.array(recup_solutions(filepath_bonus))  # Convert bonus solutions to a numpy array\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":560,"status":"ok","timestamp":1713875793856,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"Iy-_3Lv28n6V","outputId":"a2b32f6d-1cad-479a-98a5-4a260bb1a2a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," reshape_4 (Reshape)         (None, 28, 28, 1)         0         \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n","                                                                 \n"," max_pooling2d_4 (MaxPoolin  (None, 13, 13, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 11, 11, 64)        18496     \n","                                                                 \n"," max_pooling2d_5 (MaxPoolin  (None, 5, 5, 64)          0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 3, 3, 64)          36928     \n","                                                                 \n"," flatten_6 (Flatten)         (None, 576)               0         \n","                                                                 \n"," dense_26 (Dense)            (None, 64)                36928     \n","                                                                 \n"," dense_27 (Dense)            (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 93322 (364.54 KB)\n","Trainable params: 93322 (364.54 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras import layers, models\n","\n","# Define a sequential model for image classification (e.g., MNIST)\n","model_bis = models.Sequential([\n","    # Reshape input to match the expected dimensions for Conv2D layers (28x28x1)\n","    layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n","    \n","    # First convolutional layer with 32 filters and ReLU activation\n","    layers.Conv2D(32, (3, 3), activation='relu'),\n","    \n","    # Max pooling to down-sample the feature maps\n","    layers.MaxPooling2D((2, 2)),\n","    \n","    # Second convolutional layer with 64 filters and ReLU activation\n","    layers.Conv2D(64, (3, 3), activation='relu'),\n","    \n","    # Max pooling to down-sample the feature maps\n","    layers.MaxPooling2D((2, 2)),\n","    \n","    # Third convolutional layer with 64 filters and ReLU activation\n","    layers.Conv2D(64, (3, 3), activation='relu'),\n","    \n","    # Flatten the feature maps into a single vector\n","    layers.Flatten(),\n","    \n","    # Dense layer with 64 units and ReLU activation\n","    layers.Dense(64, activation='relu'),\n","    \n","    # Output layer with 10 units (for 10 classes) and softmax activation\n","    layers.Dense(10, activation='softmax')\n","])\n","\n","# Compile the model with Adam optimizer, sparse categorical crossentropy loss, and accuracy metric\n","model_bis.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Display a summary of the model architecture\n","model_bis.summary()\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":623726,"status":"ok","timestamp":1713876423690,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"yWQBMq7t8vnP","outputId":"c6fcdedd-e122-4b20-9e8c-0c60f7435b2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","1875/1875 [==============================] - 56s 29ms/step - loss: 0.1460 - accuracy: 0.9549 - val_loss: 0.0558 - val_accuracy: 0.9806\n","Epoch 2/10\n","1875/1875 [==============================] - 56s 30ms/step - loss: 0.0491 - accuracy: 0.9848 - val_loss: 0.0392 - val_accuracy: 0.9883\n","Epoch 3/10\n","1875/1875 [==============================] - 56s 30ms/step - loss: 0.0355 - accuracy: 0.9890 - val_loss: 0.0390 - val_accuracy: 0.9865\n","Epoch 4/10\n","1875/1875 [==============================] - 52s 28ms/step - loss: 0.0271 - accuracy: 0.9912 - val_loss: 0.0404 - val_accuracy: 0.9873\n","Epoch 5/10\n","1875/1875 [==============================] - 52s 28ms/step - loss: 0.0201 - accuracy: 0.9934 - val_loss: 0.0435 - val_accuracy: 0.9884\n","Epoch 6/10\n","1875/1875 [==============================] - 54s 29ms/step - loss: 0.0167 - accuracy: 0.9944 - val_loss: 0.0376 - val_accuracy: 0.9888\n","Epoch 7/10\n","1875/1875 [==============================] - 54s 29ms/step - loss: 0.0139 - accuracy: 0.9956 - val_loss: 0.0312 - val_accuracy: 0.9921\n","Epoch 8/10\n","1875/1875 [==============================] - 58s 31ms/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.0410 - val_accuracy: 0.9897\n","Epoch 9/10\n","1875/1875 [==============================] - 69s 37ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0405 - val_accuracy: 0.9897\n","Epoch 10/10\n","1875/1875 [==============================] - 72s 38ms/step - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.0412 - val_accuracy: 0.9900\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7b4a911b88e0>"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Train the model on the training data for 10 epochs and validate on the test data\n","model_bis.fit(\n","    train_images,  # Training images\n","    train_labels,  # Training labels\n","    epochs=10,  # Number of epochs for training\n","    validation_data=(test_images, test_labels)  # Validation data (test set)\n",")\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5818,"status":"ok","timestamp":1713876456093,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"qbo1qPeK8znO","outputId":"4e88cbe0-5c9f-4193-afe4-4cfc855b06ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 4s 13ms/step - loss: 0.0412 - accuracy: 0.9900\n","Test accuracy: 0.9900000095367432\n"]}],"source":["# Evaluate the model on the test data to get the loss and accuracy\n","test_loss, test_acc = model_bis.evaluate(test_images, test_labels)\n","\n","# Print the test accuracy\n","print(f\"Test accuracy: {test_acc}\")\n"]},{"cell_type":"code","execution_count":105,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65015,"status":"ok","timestamp":1713884096805,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"z1Gz9FQ780Av","outputId":"6088bc1b-19df-4c86-b798-09581c6522b5"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 5000/5000 [00:16<00:00, 307.58it/s]\n","100%|██████████| 5000/5000 [00:14<00:00, 341.54it/s]\n","100%|██████████| 5000/5000 [00:14<00:00, 340.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["157/157 [==============================] - 1s 7ms/step\n","157/157 [==============================] - 1s 7ms/step\n","157/157 [==============================] - 1s 7ms/step\n","1875/1875 [==============================] - 14s 8ms/step\n"]}],"source":["from tqdm import tqdm\n","import numpy as np\n","import tensorflow as tf\n","\n","# Function to decode the latent space and apply a threshold filter to the decoded images\n","def decoding_filtered(latent_space):\n","    \"\"\"\n","    Decode the latent space into binary images by applying a threshold of 0.5.\n","\n","    Args:\n","        latent_space (list or array): Latent space points to decode.\n","\n","    Returns:\n","        np.array: Decoded images as binary images (0.0 or 1.0).\n","    \"\"\"\n","    list_decoded = []\n","    \n","    # Iterate over each latent space point and decode it\n","    for point in tqdm(latent_space):\n","        y = tf.Variable(point)  # Convert latent space point into a TensorFlow variable\n","        new_image = decoder_function(y)  # Decode the point into an image\n","        \n","        # Apply thresholding (values > 0.5 become 1.0, otherwise 0.0)\n","        list_decoded.append(np.where(np.array(new_image) > 0.5, 1.0, 0.0))\n","    \n","    return np.array(list_decoded)\n","\n","# Main execution block\n","if __name__ == \"__main__\":\n","    # Decode and reshape the first 5000 images from different latent spaces\n","    MT_images = decoding_filtered(reparam[:5000]).reshape(5000, 28, 28, 1)  # Pr. Turinici's latent space\n","    MN_images = decoding_filtered(solutions).reshape(5000, 28, 28, 1)  # Post-gradient-descent latent space\n","    MB_images = decoding_filtered(bonus).reshape(5000, 28, 28, 1)  # Bonus latent space\n","    \n","    # Predict class probabilities for the decoded images using the trained model\n","    predictions_MT_dataset = model_bis.predict(MT_images)  # Predictions for MT_images\n","    predictions_MN_dataset = model_bis.predict(MN_images)  # Predictions for MN_images\n","    predictions_MB_dataset = model_bis.predict(MB_images)  # Predictions for MB_images\n","    predictions_mnist_dataset = model_bis.predict(train_images)  # Predictions for the original MNIST training images\n"]},{"cell_type":"code","execution_count":106,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":373,"status":"ok","timestamp":1713884118516,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"ylXxyPB6Tym7","outputId":"a98ad080-b4d4-433f-e38f-4c9be41267db"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 5000/5000 [00:00<00:00, 317899.62it/s]\n"]}],"source":["from tqdm import tqdm\n","import numpy as np\n","\n","# Function to associate the predicted class to each image\n","def associate_image_to_number(predictions):\n","    \"\"\"\n","    Convert the predicted class probabilities into the predicted number (class label).\n","\n","    Args:\n","        predictions (list or array): The class probabilities for each image.\n","\n","    Returns:\n","        list: The predicted class labels (numbers) for each image.\n","    \"\"\"\n","    result = []\n","    \n","    # Iterate over each image's predictions and find the class with the highest probability\n","    for image in tqdm(predictions):\n","        predicted_number = np.argmax(image)  # The index with the maximum value corresponds to the predicted class\n","        result.append(predicted_number)\n","    \n","    return result\n","\n","# Main execution block\n","if __name__ == \"__main__\":\n","    # Uncomment these lines to process predictions and store results\n","    \n","    # MNIST_results = associate_image_to_number(predictions_mnist_dataset)\n","    # MT_results = associate_image_to_number(predictions_MT_dataset)\n","    # MN_results = associate_image_to_number(predictions_MN_dataset)\n","    # MB_results = associate_image_to_number(predictions_MB_dataset)\n","\n","    # File paths for saving results\n","    # filepath_MNIST_results = '/content/drive/My Drive/Thesis/data/MNIST_results.txt'\n","    # filepath_MT_results = '/content/drive/My Drive/Thesis/data/MT_results.txt'\n","    # filepath_MN_results = '/content/drive/My Drive/Thesis/data/MN_results.txt'\n","    # filepath_MB_results = '/content/drive/My Drive/Thesis/data/MB_results.txt'\n","\n","    # Save the predicted class labels to respective files\n","    # create_list_txt(MNIST_results, filepath_MNIST_results)\n","    # create_list_txt(MT_results, filepath_MT_results)\n","    # create_list_txt(MN_results, filepath_MN_results)\n","    # create_list_txt(MB_results, filepath_MB_results)\n"]},{"cell_type":"code","execution_count":110,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1251,"status":"ok","timestamp":1713885876360,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"RGLajl71WqPC","outputId":"5f0c073c-cffc-401a-a26c-e5329b9042e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Your model was right 4176 out of 5000 images (ie. 0.8352 %)\n","Your model was right 4304 out of 5000 images (ie. 0.8608 %)\n","Your model was right 4184 out of 5000 images (ie. 0.8368 %)\n"]}],"source":["\n","# Define file paths for the results of each model and the ground truth labels\n","filepath_MNIST_results = '/content/drive/My Drive/Thesis/data/MNIST_results.txt'  # Ground truth labels\n","filepath_MT_results = '/content/drive/My Drive/Thesis/data/MT_results.txt'  # Predictions from model MT\n","filepath_MN_results = '/content/drive/My Drive/Thesis/data/MN_results.txt'  # Predictions from model MN\n","filepath_MB_results = '/content/drive/My Drive/Thesis/data/MB_results.txt'  # Predictions from model MB\n","\n","# Load the results from the text files into NumPy arrays\n","MNIST_results = np.array(recup_da_floats(filepath_MNIST_results))\n","MT_results = np.array(recup_da_floats(filepath_MT_results))\n","MN_results = np.array(recup_da_floats(filepath_MN_results))\n","MB_results = np.array(recup_da_floats(filepath_MB_results))\n","\n","def accuracy(ground_truth, predictions):\n","    \"\"\"\n","    Calculate the accuracy of a model's predictions compared to the ground truth labels.\n","\n","    Args:\n","        ground_truth (array-like): The true labels.\n","        predictions (array-like): The predicted labels from the model.\n","\n","    Returns:\n","        float: The accuracy of the predictions as a percentage.\n","    \"\"\"\n","    # Ensure inputs are NumPy arrays for efficient computation\n","    ground_truth = np.array(ground_truth)\n","    predictions = np.array(predictions)\n","    \n","    # Check if the lengths of the inputs match\n","    if len(ground_truth) != len(predictions):\n","        raise ValueError(\"The number of ground truth labels and predictions must be the same.\")\n","    \n","    # Calculate the number of correct predictions\n","    correct_predictions = np.sum(ground_truth == predictions)\n","    total_predictions = len(predictions)\n","    accuracy_percentage = (correct_predictions / total_predictions) * 100\n","\n","    # Print the accuracy result\n","    print(f\"Your model was right {correct_predictions} out of {total_predictions} images (i.e., {accuracy_percentage:.2f}%)\")\n","    \n","    return accuracy_percentage\n","\n","if __name__ == \"__main__\":\n","    # Calculate and print the accuracy for each model\n","    print(\"Accuracy of MT model:\")\n","    accuracy_MT = accuracy(MNIST_results, MT_results)\n","    \n","    print(\"\\nAccuracy of MN model:\")\n","    accuracy_MN = accuracy(MNIST_results, MN_results)\n","    \n","    print(\"\\nAccuracy of MB model:\")\n","    accuracy_MB = accuracy(MNIST_results, MB_results)\n"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":470},"executionInfo":{"elapsed":782,"status":"ok","timestamp":1713881913452,"user":{"displayName":"benjamin krief","userId":"13323366122837332189"},"user_tz":-120},"id":"lmpneTjND5zD","outputId":"77de5883-31b4-4320-aef8-53ced70944fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["La classe prédite pour l'image n°555 est: 2\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Example index for the MNIST dataset\n","mnist_index = 555\n","\n","# Get the predicted class by finding the index of the maximum probability\n","predicted_class = np.argmax(predictions_mnist_dataset[mnist_index])  # Indices start at 0 in Python\n","\n","# Print the predicted class for the specified MNIST image\n","print(f\"La classe prédite pour l'image n°{mnist_index} est: {predicted_class}\")\n","\n","# Display the original MNIST image\n","plt.imshow(train_images[mnist_index], cmap=plt.cm.binary)\n","plt.title(f\"Classe prédite: {predicted_class}\")\n","plt.axis('off')  # Optional: Turn off axis labels for better visualization\n","plt.show()\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPWT7S5NBJ4lGmWRN0QXqJL","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
